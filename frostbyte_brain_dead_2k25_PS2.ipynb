{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-03-22T15:38:40.550584Z",
     "iopub.status.busy": "2025-03-22T15:38:40.550185Z",
     "iopub.status.idle": "2025-03-22T15:38:40.557847Z",
     "shell.execute_reply": "2025-03-22T15:38:40.557000Z",
     "shell.execute_reply.started": "2025-03-22T15:38:40.550521Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/comspscholar-dataset/Brain Dead CompScholar Dataset.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-22T15:38:43.130878Z",
     "iopub.status.busy": "2025-03-22T15:38:43.130512Z",
     "iopub.status.idle": "2025-03-22T15:38:43.184479Z",
     "shell.execute_reply": "2025-03-22T15:38:43.183568Z",
     "shell.execute_reply.started": "2025-03-22T15:38:43.130847Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Paper Id</th>\n",
       "      <th>Paper Title</th>\n",
       "      <th>Key Words</th>\n",
       "      <th>Abstract</th>\n",
       "      <th>Conclusion</th>\n",
       "      <th>Document</th>\n",
       "      <th>Paper Type</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Topic</th>\n",
       "      <th>OCR</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Multi-document Summarization via Deep Learning...</td>\n",
       "      <td>Multi-document summarization (MDS), Deep learn...</td>\n",
       "      <td>Multi-document summarization (MDS) is an effec...</td>\n",
       "      <td>In this article, we have presented the first c...</td>\n",
       "      <td>Multi-document Summarization via Deep Learning...</td>\n",
       "      <td>Text summarization</td>\n",
       "      <td>This article presents a systematic overview of...</td>\n",
       "      <td>Natural Language Processing</td>\n",
       "      <td>lla i aye RR | poe [Sena Sena | Sena, —+ ar ea...</td>\n",
       "      <td>Deep Learning and Machine Learning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>NLP based Machine Learning Approaches for Text...</td>\n",
       "      <td>Text summarization, Abstractive and extractive...</td>\n",
       "      <td>Due to the plethora of data available today, t...</td>\n",
       "      <td>We have seen that due to abundant availability...</td>\n",
       "      <td>NLP based Machine Learning Approaches for Text...</td>\n",
       "      <td>Natural Language Processing</td>\n",
       "      <td>The article discusses the importance of text s...</td>\n",
       "      <td>Natural Language Processing</td>\n",
       "      <td>@STOM © Word Vector Embedding kenearest neighb...</td>\n",
       "      <td>Deep Learning and Machine Learning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Abstractive text summarization using LSTM-CNN ...</td>\n",
       "      <td>Text mining . Abstractive text summarization ....</td>\n",
       "      <td>Abstractive Text Summarization (ATS), which i...</td>\n",
       "      <td>In this paper, we develop a novel LSTM-CNN bas...</td>\n",
       "      <td>Abstractive text summarization using LSTM-CNN ...</td>\n",
       "      <td>Text summarization</td>\n",
       "      <td>The article presents a new framework for abstr...</td>\n",
       "      <td>Natural Language Processing</td>\n",
       "      <td>encoder decoderWord Merpholosical Coreterence ...</td>\n",
       "      <td>Deep Learning and Machine Learning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>DEXPERTS: Decoding-Time Controlled Text Genera...</td>\n",
       "      <td>Natural language generation, Controlled text g...</td>\n",
       "      <td>Despite recent advances in natural language\\ng...</td>\n",
       "      <td>We present DEXPERTS, a method for controlled\\n...</td>\n",
       "      <td>DEXPERTS: Decoding-Time Controlled Text Genera...</td>\n",
       "      <td>Text generation</td>\n",
       "      <td>The paper proposes a method called DEXPERTS fo...</td>\n",
       "      <td>Natural Language Processing</td>\n",
       "      <td>reatva star on negative proms oe TT os ee oe S...</td>\n",
       "      <td>Deep Learning and Machine Learning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>A Survey of Knowledge-enhanced Text Generation</td>\n",
       "      <td>text-to-text generation, natural language proc...</td>\n",
       "      <td>The goal of text-to-text generation is to make...</td>\n",
       "      <td>In this survey, we present a comprehensive rev...</td>\n",
       "      <td>A Survey of Knowledge-enhanced Text Generation...</td>\n",
       "      <td>Text generation</td>\n",
       "      <td>The paper discusses the challenges in text-to-...</td>\n",
       "      <td>Natural Language Processing</td>\n",
       "      <td>(ira =&gt; Generation model =&gt; foam] | Generation...</td>\n",
       "      <td>Deep Learning and Machine Learning</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Paper Id                                        Paper Title  \\\n",
       "0         1  Multi-document Summarization via Deep Learning...   \n",
       "1         2  NLP based Machine Learning Approaches for Text...   \n",
       "2         3  Abstractive text summarization using LSTM-CNN ...   \n",
       "3         4  DEXPERTS: Decoding-Time Controlled Text Genera...   \n",
       "4         5     A Survey of Knowledge-enhanced Text Generation   \n",
       "\n",
       "                                           Key Words  \\\n",
       "0  Multi-document summarization (MDS), Deep learn...   \n",
       "1  Text summarization, Abstractive and extractive...   \n",
       "2  Text mining . Abstractive text summarization ....   \n",
       "3  Natural language generation, Controlled text g...   \n",
       "4  text-to-text generation, natural language proc...   \n",
       "\n",
       "                                            Abstract  \\\n",
       "0  Multi-document summarization (MDS) is an effec...   \n",
       "1  Due to the plethora of data available today, t...   \n",
       "2   Abstractive Text Summarization (ATS), which i...   \n",
       "3  Despite recent advances in natural language\\ng...   \n",
       "4  The goal of text-to-text generation is to make...   \n",
       "\n",
       "                                          Conclusion  \\\n",
       "0  In this article, we have presented the first c...   \n",
       "1  We have seen that due to abundant availability...   \n",
       "2  In this paper, we develop a novel LSTM-CNN bas...   \n",
       "3  We present DEXPERTS, a method for controlled\\n...   \n",
       "4  In this survey, we present a comprehensive rev...   \n",
       "\n",
       "                                            Document  \\\n",
       "0  Multi-document Summarization via Deep Learning...   \n",
       "1  NLP based Machine Learning Approaches for Text...   \n",
       "2  Abstractive text summarization using LSTM-CNN ...   \n",
       "3  DEXPERTS: Decoding-Time Controlled Text Genera...   \n",
       "4  A Survey of Knowledge-enhanced Text Generation...   \n",
       "\n",
       "                    Paper Type  \\\n",
       "0           Text summarization   \n",
       "1  Natural Language Processing   \n",
       "2           Text summarization   \n",
       "3              Text generation   \n",
       "4              Text generation   \n",
       "\n",
       "                                             Summary  \\\n",
       "0  This article presents a systematic overview of...   \n",
       "1  The article discusses the importance of text s...   \n",
       "2  The article presents a new framework for abstr...   \n",
       "3  The paper proposes a method called DEXPERTS fo...   \n",
       "4  The paper discusses the challenges in text-to-...   \n",
       "\n",
       "                         Topic  \\\n",
       "0  Natural Language Processing   \n",
       "1  Natural Language Processing   \n",
       "2  Natural Language Processing   \n",
       "3  Natural Language Processing   \n",
       "4  Natural Language Processing   \n",
       "\n",
       "                                                 OCR  \\\n",
       "0  lla i aye RR | poe [Sena Sena | Sena, —+ ar ea...   \n",
       "1  @STOM © Word Vector Embedding kenearest neighb...   \n",
       "2  encoder decoderWord Merpholosical Coreterence ...   \n",
       "3  reatva star on negative proms oe TT os ee oe S...   \n",
       "4  (ira => Generation model => foam] | Generation...   \n",
       "\n",
       "                               labels  \n",
       "0  Deep Learning and Machine Learning  \n",
       "1  Deep Learning and Machine Learning  \n",
       "2  Deep Learning and Machine Learning  \n",
       "3  Deep Learning and Machine Learning  \n",
       "4  Deep Learning and Machine Learning  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=pd.read_csv('/kaggle/input/comspscholar-dataset/Brain Dead CompScholar Dataset.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = data.loc[0, \"Summary\"]\n",
    "abstract = data.loc[0, \"Abstract\"]\n",
    "\n",
    "# Printing the values\n",
    "print(\"Summary:\", summary)\n",
    "print(\"Abstract:\", abstract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "# Ensure nltk's tokenizer is available\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "tokens = word_tokenize(data.loc[266, \"Summary\"])\n",
    "\n",
    "# Counting the number of tokens\n",
    "token_count = len(tokens)\n",
    "\n",
    "print(\"Number of tokens in Summary:\", token_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row=data.loc[0]\n",
    "text = \" \".join(row[[\"Paper Title\", \"Abstract\", \"Key Words\", \"Document\", \"Conclusion\", \"Paper Type\"]].astype(str))\n",
    "tokens=word_tokenize(text)\n",
    "token_count = len(tokens)\n",
    "\n",
    "print(\"Number of tokens in Summary:\", token_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxi=0\n",
    "pos=0\n",
    "for index, row in data.iterrows():\n",
    "    # text = \" \".join(row[[\"Paper Title\", \"Abstract\", \"Key Words\", \"Document\", \"Conclusion\", \"Paper Type\"]].astype(str))\n",
    "    text=row[\"Summary\"]\n",
    "    tokens=word_tokenize(text)\n",
    "    token_count = len(tokens)\n",
    "    # if(token_count>maxi):\n",
    "    #     maxi=token_count\n",
    "    #     pos=index\n",
    "    maxi+=token_count\n",
    "print(maxi/371)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-22T16:27:00.096466Z",
     "iopub.status.busy": "2025-03-22T16:27:00.096121Z",
     "iopub.status.idle": "2025-03-22T16:27:04.686566Z",
     "shell.execute_reply": "2025-03-22T16:27:04.685232Z",
     "shell.execute_reply.started": "2025-03-22T16:27:00.096438Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting evaluate\n",
      "  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\n",
      "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.3.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from evaluate) (1.26.4)\n",
      "Requirement already satisfied: dill in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from evaluate) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.70.16)\n",
      "Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.12.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.29.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from evaluate) (24.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.17.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (19.0.1)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.11.12)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (6.0.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.12.2)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->evaluate) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->evaluate) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->evaluate) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->evaluate) (2025.0.1)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->evaluate) (2022.0.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->evaluate) (2.4.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2025.1.31)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2025.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.4.6)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (25.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.18.3)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\n",
      "Requirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->evaluate) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->evaluate) (2022.0.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->evaluate) (1.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17->evaluate) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17->evaluate) (2024.2.0)\n",
      "Downloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: evaluate\n",
      "Successfully installed evaluate-0.4.3\n"
     ]
    }
   ],
   "source": [
    "!pip install evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-22T16:27:04.688897Z",
     "iopub.status.busy": "2025-03-22T16:27:04.688504Z",
     "iopub.status.idle": "2025-03-22T16:27:10.870919Z",
     "shell.execute_reply": "2025-03-22T16:27:10.869950Z",
     "shell.execute_reply.started": "2025-03-22T16:27:04.688859Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting rouge_score\n",
      "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.4.0)\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from rouge_score) (3.2.4)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.26.4)\n",
      "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.17.0)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy->rouge_score) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy->rouge_score) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy->rouge_score) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy->rouge_score) (2025.0.1)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy->rouge_score) (2022.0.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy->rouge_score) (2.4.1)\n",
      "Requirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->rouge_score) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->rouge_score) (2022.0.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy->rouge_score) (1.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy->rouge_score) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy->rouge_score) (2024.2.0)\n",
      "Building wheels for collected packages: rouge_score\n",
      "  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24935 sha256=d246b3cd6452dbe1ec3262ba26665042f0e149eba28a903041b01d404cea5967\n",
      "  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\n",
      "Successfully built rouge_score\n",
      "Installing collected packages: rouge_score\n",
      "Successfully installed rouge_score-0.1.2\n"
     ]
    }
   ],
   "source": [
    "!pip install rouge_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-22T14:07:38.654590Z",
     "iopub.status.busy": "2025-03-22T14:07:38.654360Z",
     "iopub.status.idle": "2025-03-22T14:07:45.449649Z",
     "shell.execute_reply": "2025-03-22T14:07:45.448746Z",
     "shell.execute_reply.started": "2025-03-22T14:07:38.654570Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline, set_seed\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import evaluate\n",
    "\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-22T14:07:45.451627Z",
     "iopub.status.busy": "2025-03-22T14:07:45.450961Z",
     "iopub.status.idle": "2025-03-22T14:07:45.476838Z",
     "shell.execute_reply": "2025-03-22T14:07:45.475963Z",
     "shell.execute_reply.started": "2025-03-22T14:07:45.451592Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-22T14:07:45.494385Z",
     "iopub.status.busy": "2025-03-22T14:07:45.494108Z",
     "iopub.status.idle": "2025-03-22T14:07:54.378108Z",
     "shell.execute_reply": "2025-03-22T14:07:54.377083Z",
     "shell.execute_reply.started": "2025-03-22T14:07:45.494362Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-cnn_dailymail and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_ckpt = \"google/pegasus-cnn_dailymail\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
    "\n",
    "model_pegasus = AutoModelForSeq2SeqLM.from_pretrained(model_ckpt).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-22T14:07:45.479043Z",
     "iopub.status.busy": "2025-03-22T14:07:45.478698Z",
     "iopub.status.idle": "2025-03-22T14:07:45.493226Z",
     "shell.execute_reply": "2025-03-22T14:07:45.492324Z",
     "shell.execute_reply.started": "2025-03-22T14:07:45.479020Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-22T14:16:57.060480Z",
     "iopub.status.busy": "2025-03-22T14:16:57.060165Z",
     "iopub.status.idle": "2025-03-22T14:16:57.067558Z",
     "shell.execute_reply": "2025-03-22T14:16:57.066454Z",
     "shell.execute_reply.started": "2025-03-22T14:16:57.060457Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "def generate_batch_sized_chunks(list_of_elements, batch_size):\n",
    "    \"\"\"\n",
    "    Split the dataset into smaller batches that we can process simultaneously.\n",
    "    Yield successive batch-sized chunks from list_of_elements.\n",
    "    \"\"\"\n",
    "    for i in range(0, len(list_of_elements), batch_size):\n",
    "        yield list_of_elements[i : i + batch_size]\n",
    "\n",
    "def calculate_metric_on_test_ds(dataset, metric, model, tokenizer, batch_size=16, device=\"cuda\"):\n",
    "    \"\"\"\n",
    "    Compute the evaluation metric for the dataset.\n",
    "    \"\"\"\n",
    "    # Preparing input and target texts\n",
    "    dataset_inputs = dataset[\"Text\"].fillna(\" \").tolist()\n",
    "    dataset_targets = dataset[\"Summary\"].fillna(\" \").tolist()  # Ensure summaries are valid strings\n",
    "    \n",
    "    # Splitting into batches\n",
    "    input_batches = list(generate_batch_sized_chunks(dataset_inputs, batch_size))\n",
    "    target_batches = list(generate_batch_sized_chunks(dataset_targets, batch_size))\n",
    "\n",
    "    # Processing each batch\n",
    "    for input_batch, target_batch in tqdm(zip(input_batches, target_batches), \n",
    "                                          total=len(input_batches), desc=\"Processing Batches\"):\n",
    "\n",
    "        # Tokenizing the input text\n",
    "        inputs = tokenizer(input_batch, max_length=1024, truncation=True,\n",
    "                           padding=\"max_length\", return_tensors=\"pt\")\n",
    "\n",
    "        # Generating summaries\n",
    "        with torch.no_grad():  # Disable gradient computation for inference\n",
    "            summaries = model.generate(input_ids=inputs[\"input_ids\"].to(device),\n",
    "                                       attention_mask=inputs[\"attention_mask\"].to(device),\n",
    "                                       length_penalty=0.8, num_beams=8, max_length=128)\n",
    "\n",
    "        # Decoding the summaries\n",
    "        decoded_summaries = [tokenizer.decode(s, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "                             for s in summaries]\n",
    "\n",
    "        # Updating the metric\n",
    "        metric.add_batch(predictions=decoded_summaries, references=target_batch)\n",
    "\n",
    "    # Compute and return the final scores\n",
    "    score = metric.compute()\n",
    "    return score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-22T14:07:58.192395Z",
     "iopub.status.busy": "2025-03-22T14:07:58.192049Z",
     "iopub.status.idle": "2025-03-22T14:07:58.257414Z",
     "shell.execute_reply": "2025-03-22T14:07:58.256465Z",
     "shell.execute_reply.started": "2025-03-22T14:07:58.192362Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import datasets\n",
    "\n",
    "cols = [\"Paper Title\", \"Key Words\", \"Abstract\", \"Conclusion\", \"Document\", \"Paper Type\", \"Topic\"]\n",
    "\n",
    "# Ensure missing values are replaced with empty strings before concatenation\n",
    "data[\"Text\"] = data[cols].fillna(\" \").agg(\" \".join, axis=1)\n",
    "data=data[[\"Text\", \"Summary\"]]\n",
    "# Splitting data into train (70%), validation (15%), and test (15%)\n",
    "train_data, temp_data = train_test_split(data, test_size=0.3, random_state=42)\n",
    "val_data, test_data = train_test_split(temp_data, test_size=0.5, random_state=42)\n",
    "\n",
    "# Convert to Hugging Face Dataset format\n",
    "train_dataset = datasets.Dataset.from_pandas(train_data)\n",
    "val_dataset = datasets.Dataset.from_pandas(val_data)\n",
    "test_dataset = datasets.Dataset.from_pandas(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-22T14:08:17.883472Z",
     "iopub.status.busy": "2025-03-22T14:08:17.883109Z",
     "iopub.status.idle": "2025-03-22T14:08:18.861071Z",
     "shell.execute_reply": "2025-03-22T14:08:18.860363Z",
     "shell.execute_reply.started": "2025-03-22T14:08:17.883441Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44a4d7d48ee04acebb2017cf2cc3bbc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/259 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:3953: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77dc4f1fa1d340edb247c50185a02ba5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/56 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b15ad57fd8e4029a39445b4407ad8f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/56 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def convert_examples_to_features(example_batch):\n",
    "    input_encodings = tokenizer(example_batch['Text'], max_length=1024, truncation=True)\n",
    "\n",
    "# Tokenizing the Target Summaries:\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        target_encodings = tokenizer(example_batch['Summary'], max_length = 256, truncation = True )\n",
    "\n",
    "# Returning the Tokenized Features:\n",
    "    return {\n",
    "        'input_ids' : input_encodings['input_ids'],\n",
    "        'attention_mask': input_encodings['attention_mask'],\n",
    "        'labels': target_encodings['input_ids']\n",
    "    }\n",
    "\n",
    "# Apply the function to datasets\n",
    "train_dataset = train_dataset.map(convert_examples_to_features, batched=True)\n",
    "val_dataset = val_dataset.map(convert_examples_to_features, batched=True)\n",
    "test_dataset = test_dataset.map(convert_examples_to_features, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-22T14:08:22.280634Z",
     "iopub.status.busy": "2025-03-22T14:08:22.280325Z",
     "iopub.status.idle": "2025-03-22T14:08:22.286962Z",
     "shell.execute_reply": "2025-03-22T14:08:22.285910Z",
     "shell.execute_reply.started": "2025-03-22T14:08:22.280611Z"
    }
   },
   "outputs": [],
   "source": [
    "columns = [\"input_ids\", \"labels\", \"attention_mask\"]\n",
    "train_dataset.set_format(type=\"torch\", columns=columns)\n",
    "val_dataset.set_format(type=\"torch\", columns=columns)\n",
    "test_dataset.set_format(type=\"torch\", columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-22T14:08:24.625311Z",
     "iopub.status.busy": "2025-03-22T14:08:24.625018Z",
     "iopub.status.idle": "2025-03-22T14:08:24.629614Z",
     "shell.execute_reply": "2025-03-22T14:08:24.628533Z",
     "shell.execute_reply.started": "2025-03-22T14:08:24.625290Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "seq2seq_data_collator = DataCollatorForSeq2Seq(tokenizer, model=model_pegasus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-22T14:24:45.775162Z",
     "iopub.status.busy": "2025-03-22T14:24:45.774822Z",
     "iopub.status.idle": "2025-03-22T14:24:45.805986Z",
     "shell.execute_reply": "2025-03-22T14:24:45.805116Z",
     "shell.execute_reply.started": "2025-03-22T14:24:45.775139Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments( output_dir=\"pegasus-samsum\",\n",
    "                                 num_train_epochs=10,\n",
    "                                 warmup_steps=500,\n",
    "                                 per_device_train_batch_size=1,\n",
    "                                 per_gpu_eval_batch_size=1,\n",
    "                                 weight_decay=0.01,\n",
    "                                 logging_steps=10, \n",
    "                                 push_to_hub=True,\n",
    "                                 evaluation_strategy='steps', \n",
    "                                 eval_steps=500,\n",
    "                                 save_steps=1e6,\n",
    "                                 gradient_accumulation_steps=16,\n",
    "                                 report_to=\"none\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-22T14:08:35.647876Z",
     "iopub.status.busy": "2025-03-22T14:08:35.647508Z",
     "iopub.status.idle": "2025-03-22T14:08:35.670074Z",
     "shell.execute_reply": "2025-03-22T14:08:35.669240Z",
     "shell.execute_reply.started": "2025-03-22T14:08:35.647845Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "96103"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-22T14:24:52.254772Z",
     "iopub.status.busy": "2025-03-22T14:24:52.254423Z",
     "iopub.status.idle": "2025-03-22T14:24:52.538730Z",
     "shell.execute_reply": "2025-03-22T14:24:52.538041Z",
     "shell.execute_reply.started": "2025-03-22T14:24:52.254736Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-27-8ca8c94c3db3>:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(model=model_pegasus,\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(model=model_pegasus, \n",
    "                 args=training_args,\n",
    "                 tokenizer=tokenizer,\n",
    "                 data_collator=seq2seq_data_collator,\n",
    "                 train_dataset=train_dataset,\n",
    "                 eval_dataset=val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-22T14:24:55.750553Z",
     "iopub.status.busy": "2025-03-22T14:24:55.750120Z",
     "iopub.status.idle": "2025-03-22T14:42:37.905613Z",
     "shell.execute_reply": "2025-03-22T14:42:37.904822Z",
     "shell.execute_reply.started": "2025-03-22T14:24:55.750510Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='160' max='160' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [160/160 15:52, Epoch 9/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=160, training_loss=1.7023686051368714, metrics={'train_runtime': 957.9728, 'train_samples_per_second': 2.704, 'train_steps_per_second': 0.167, 'total_flos': 6320455092731904.0, 'train_loss': 1.7023686051368714, 'epoch': 9.432432432432432})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-22T14:47:16.429720Z",
     "iopub.status.busy": "2025-03-22T14:47:16.429347Z",
     "iopub.status.idle": "2025-03-22T14:48:26.744411Z",
     "shell.execute_reply": "2025-03-22T14:48:26.743420Z",
     "shell.execute_reply.started": "2025-03-22T14:47:16.429679Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|██████████| 14/14 [01:08<00:00,  4.90s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge1': 0.4926840042646487, 'rouge2': 0.2624993226301624, 'rougeL': 0.35761643289405654, 'rougeLsum': 0.3576101533213595}\n"
     ]
    }
   ],
   "source": [
    "#model with fine-tuning\n",
    "metric = evaluate.load(\"rouge\")\n",
    "score = calculate_metric_on_test_ds(test_data, metric, trainer.model, tokenizer, batch_size=4, device=device)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-22T14:19:30.751094Z",
     "iopub.status.busy": "2025-03-22T14:19:30.750759Z",
     "iopub.status.idle": "2025-03-22T14:20:25.478425Z",
     "shell.execute_reply": "2025-03-22T14:20:25.477439Z",
     "shell.execute_reply.started": "2025-03-22T14:19:30.751070Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|██████████| 14/14 [00:53<00:00,  3.80s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge1': 0.34926607845636115, 'rouge2': 0.17728131279827714, 'rougeL': 0.2512846517424161, 'rougeLsum': 0.2505077608020769}\n"
     ]
    }
   ],
   "source": [
    "#model without fine tuning\n",
    "metric = evaluate.load(\"rouge\")\n",
    "score = calculate_metric_on_test_ds(test_data, metric, model_pegasus, tokenizer, batch_size=4, device=device)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-22T14:52:38.467999Z",
     "iopub.status.busy": "2025-03-22T14:52:38.467574Z",
     "iopub.status.idle": "2025-03-22T14:56:09.432885Z",
     "shell.execute_reply": "2025-03-22T14:56:09.431691Z",
     "shell.execute_reply.started": "2025-03-22T14:52:38.467964Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  adding: kaggle/working/ (stored 0%)\n",
      "  adding: kaggle/working/.virtual_documents/ (stored 0%)\n",
      "  adding: kaggle/working/pegasus-samsum/ (stored 0%)\n",
      "  adding: kaggle/working/pegasus-samsum/checkpoint-160/ (stored 0%)\n",
      "  adding: kaggle/working/pegasus-samsum/checkpoint-160/optimizer.pt^C\n",
      "\n",
      "\n",
      "\n",
      "zip error: Interrupted (aborting)\n"
     ]
    }
   ],
   "source": [
    "!zip -r /kaggle/working/output.zip /kaggle/working/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import FileLink\n",
    "FileLink('/kaggle/working/output.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-22T15:53:59.647309Z",
     "iopub.status.busy": "2025-03-22T15:53:59.646931Z",
     "iopub.status.idle": "2025-03-22T15:53:59.754781Z",
     "shell.execute_reply": "2025-03-22T15:53:59.754033Z",
     "shell.execute_reply.started": "2025-03-22T15:53:59.647283Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import datasets\n",
    "\n",
    "data=pd.read_csv('/kaggle/input/comspscholar-dataset/Brain Dead CompScholar Dataset.csv')\n",
    "cols = [\"Paper Title\", \"Key Words\", \"Abstract\", \"Conclusion\", \"Document\", \"Paper Type\", \"Topic\"]\n",
    "\n",
    "# Ensure missing values are replaced with empty strings before concatenation\n",
    "data[\"Text\"] = data[cols].fillna(\" \").agg(\" \".join, axis=1)\n",
    "data=data[[\"Text\", \"Summary\"]]\n",
    "# Splitting data into train (70%), validation (15%), and test (15%)\n",
    "train_data, temp_data = train_test_split(data, test_size=0.3, random_state=42)\n",
    "val_data, test_data = train_test_split(temp_data, test_size=0.5, random_state=42)\n",
    "\n",
    "# Convert to Hugging Face Dataset format\n",
    "train_dataset = datasets.Dataset.from_pandas(train_data)\n",
    "val_dataset = datasets.Dataset.from_pandas(val_data)\n",
    "test_dataset = datasets.Dataset.from_pandas(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-22T15:54:09.445349Z",
     "iopub.status.busy": "2025-03-22T15:54:09.445033Z",
     "iopub.status.idle": "2025-03-22T15:54:09.454101Z",
     "shell.execute_reply": "2025-03-22T15:54:09.453273Z",
     "shell.execute_reply.started": "2025-03-22T15:54:09.445326Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Multi-document Summarization via Deep Learning...</td>\n",
       "      <td>This article presents a systematic overview of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NLP based Machine Learning Approaches for Text...</td>\n",
       "      <td>The article discusses the importance of text s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Abstractive text summarization using LSTM-CNN ...</td>\n",
       "      <td>The article presents a new framework for abstr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DEXPERTS: Decoding-Time Controlled Text Genera...</td>\n",
       "      <td>The paper proposes a method called DEXPERTS fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A Survey of Knowledge-enhanced Text Generation...</td>\n",
       "      <td>The paper discusses the challenges in text-to-...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  \\\n",
       "0  Multi-document Summarization via Deep Learning...   \n",
       "1  NLP based Machine Learning Approaches for Text...   \n",
       "2  Abstractive text summarization using LSTM-CNN ...   \n",
       "3  DEXPERTS: Decoding-Time Controlled Text Genera...   \n",
       "4  A Survey of Knowledge-enhanced Text Generation...   \n",
       "\n",
       "                                             Summary  \n",
       "0  This article presents a systematic overview of...  \n",
       "1  The article discusses the importance of text s...  \n",
       "2  The article presents a new framework for abstr...  \n",
       "3  The paper proposes a method called DEXPERTS fo...  \n",
       "4  The paper discusses the challenges in text-to-...  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-22T15:39:13.204414Z",
     "iopub.status.busy": "2025-03-22T15:39:13.204076Z",
     "iopub.status.idle": "2025-03-22T15:40:37.690850Z",
     "shell.execute_reply": "2025-03-22T15:40:37.689573Z",
     "shell.execute_reply.started": "2025-03-22T15:39:13.204390Z"
    }
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install pip3-autoremove\n",
    "!pip-autoremove torch torchvision torchaudio -y\n",
    "!pip install torch torchvision torchaudio xformers triton\n",
    "!pip install \"unsloth[kaggle-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "\n",
    "import os\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-22T15:47:35.458068Z",
     "iopub.status.busy": "2025-03-22T15:47:35.457702Z",
     "iopub.status.idle": "2025-03-22T15:47:39.701075Z",
     "shell.execute_reply": "2025-03-22T15:47:39.700119Z",
     "shell.execute_reply.started": "2025-03-22T15:47:35.458041Z"
    }
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install xformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-22T17:22:59.098881Z",
     "iopub.status.busy": "2025-03-22T17:22:59.098416Z",
     "iopub.status.idle": "2025-03-22T17:22:59.105569Z",
     "shell.execute_reply": "2025-03-22T17:22:59.104762Z",
     "shell.execute_reply.started": "2025-03-22T17:22:59.098849Z"
    }
   },
   "outputs": [],
   "source": [
    "# For dataset\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import ast\n",
    "import re\n",
    "import numpy as np\n",
    "from datasets import Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# For LLM\n",
    "from peft import LoraConfig, PeftModel\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    set_seed,\n",
    "    pipeline\n",
    ")\n",
    "from trl import SFTTrainer, setup_chat_format, SFTConfig\n",
    "\n",
    "import torch\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-22T17:23:01.845837Z",
     "iopub.status.busy": "2025-03-22T17:23:01.845442Z",
     "iopub.status.idle": "2025-03-22T17:23:01.849795Z",
     "shell.execute_reply": "2025-03-22T17:23:01.848878Z",
     "shell.execute_reply.started": "2025-03-22T17:23:01.845807Z"
    }
   },
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-22T17:23:07.888260Z",
     "iopub.status.busy": "2025-03-22T17:23:07.887916Z",
     "iopub.status.idle": "2025-03-22T17:23:24.003981Z",
     "shell.execute_reply": "2025-03-22T17:23:24.002983Z",
     "shell.execute_reply.started": "2025-03-22T17:23:07.888226Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.3.18: Fast Llama patching. Transformers: 4.50.0.\n",
      "   \\\\   /|    Tesla P100-PCIE-16GB. Num GPUs = 1. Max memory: 15.888 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 6.0. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "max_seq_length = 4096 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "# model_path=\"/kaggle/input/fine-tuned-llama-model\"\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "#     model_name = \"/kaggle/input/llama-3.1/transformers/8b-instruct/2\",\n",
    "    model_name=\"unsloth/Meta-Llama-3.1-8B\",\n",
    "    # model_name = model_path,\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-22T17:23:24.005509Z",
     "iopub.status.busy": "2025-03-22T17:23:24.005176Z",
     "iopub.status.idle": "2025-03-22T17:23:30.548938Z",
     "shell.execute_reply": "2025-03-22T17:23:30.547925Z",
     "shell.execute_reply.started": "2025-03-22T17:23:24.005477Z"
    }
   },
   "outputs": [],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-22T17:24:25.651419Z",
     "iopub.status.busy": "2025-03-22T17:24:25.651058Z",
     "iopub.status.idle": "2025-03-22T17:24:25.694861Z",
     "shell.execute_reply": "2025-03-22T17:24:25.693844Z",
     "shell.execute_reply.started": "2025-03-22T17:24:25.651385Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92db3c76fd984800a1f64e98f88537fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/259 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "prompt=\"\"\"Below is an instruction that describes a task, Paired with an input\n",
    "### Instruction:\n",
    "Analyze the research article content and give me a summary from the research article. The summary should be as precise as possible and must include all the important points. Donot repeat information. Make the summary as crisp as possible\n",
    "\n",
    "### Input:\n",
    "{}\n",
    "### Response: \n",
    "{}\n",
    "\"\"\"\n",
    "\n",
    "EOS_TOKEN=tokenizer.eos_token\n",
    "def formatting_prompts_func(row):\n",
    "    inputs =row[\"Text\"]\n",
    "    outputs = row[\"Summary\"]\n",
    "    texts = []\n",
    "    \n",
    "    for input, output in zip(inputs, outputs):\n",
    "        text = prompt.format(input, output) + EOS_TOKEN\n",
    "        texts.append(text)\n",
    "        \n",
    "    return { \"texts\" : texts, }\n",
    "\n",
    "train_dataset = train_dataset.map(formatting_prompts_func, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-22T17:24:59.054684Z",
     "iopub.status.busy": "2025-03-22T17:24:59.054341Z",
     "iopub.status.idle": "2025-03-22T17:25:01.916521Z",
     "shell.execute_reply": "2025-03-22T17:25:01.915755Z",
     "shell.execute_reply.started": "2025-03-22T17:24:59.054658Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce04c54b3b774177baf6421fe5bb43e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Unsloth: Tokenizing [\"texts\"] (num_proc=2):   0%|          | 0/259 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = train_dataset,\n",
    "    dataset_text_field = \"texts\",\n",
    "    max_seq_length = 1024,\n",
    "    dataset_num_proc = 2,\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = 2,\n",
    "        gradient_accumulation_steps = 4,\n",
    "        num_train_epochs = 1,\n",
    "        learning_rate = 2e-4,\n",
    "        max_steps = 35,\n",
    "        fp16 = not is_bfloat16_supported(),\n",
    "        bf16 = is_bfloat16_supported(),\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"outputs\",\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-22T17:25:06.026637Z",
     "iopub.status.busy": "2025-03-22T17:25:06.026299Z",
     "iopub.status.idle": "2025-03-22T17:25:06.033813Z",
     "shell.execute_reply": "2025-03-22T17:25:06.032813Z",
     "shell.execute_reply.started": "2025-03-22T17:25:06.026610Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU = Tesla P100-PCIE-16GB. Max memory = 15.888 GB.\n",
      "12.521 GB of memory reserved.\n"
     ]
    }
   ],
   "source": [
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-22T17:25:14.220763Z",
     "iopub.status.busy": "2025-03-22T17:25:14.220395Z",
     "iopub.status.idle": "2025-03-22T17:54:25.532566Z",
     "shell.execute_reply": "2025-03-22T17:54:25.531628Z",
     "shell.execute_reply.started": "2025-03-22T17:25:14.220732Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 259 | Num Epochs = 2 | Total steps = 35\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 4\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8\n",
      " \"-____-\"     Trainable parameters = 41,943,040/8,000,000,000 (0.52% trained)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='35' max='35' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [35/35 28:17, Epoch 1/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.410200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.439800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.636100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.586500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.388900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.437700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.255400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.103200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.319700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.464800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>1.413000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>1.137100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>1.324000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>1.170000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>1.351000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>1.319200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>1.402800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>1.524800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>1.298400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.151600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>1.264900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>1.114900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>1.229700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>1.293200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>1.340600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>1.262900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>1.526100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>1.217400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>1.299900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.274000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>0.902900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>1.327200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>1.349700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>1.039100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>1.125100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-22T17:54:25.534114Z",
     "iopub.status.busy": "2025-03-22T17:54:25.533848Z",
     "iopub.status.idle": "2025-03-22T17:54:26.400180Z",
     "shell.execute_reply": "2025-03-22T17:54:26.399378Z",
     "shell.execute_reply.started": "2025-03-22T17:54:25.534089Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('lora_model/tokenizer_config.json',\n",
       " 'lora_model/special_tokens_map.json',\n",
       " 'lora_model/tokenizer.json')"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.model.save_pretrained(\"lora_model\") # Local saving\n",
    "tokenizer.save_pretrained(\"lora_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-03-22T16:06:51.412744Z",
     "iopub.status.busy": "2025-03-22T16:06:51.412340Z",
     "iopub.status.idle": "2025-03-22T16:06:51.419197Z",
     "shell.execute_reply": "2025-03-22T16:06:51.418238Z",
     "shell.execute_reply.started": "2025-03-22T16:06:51.412714Z"
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Multi-document Summarization via Deep Learning\\nTechniques: A Survey Multi-document summarization (MDS), Deep learning models, Objective functions, Taxonomy, Evaluation metrics, Future directions, Information aggregation Multi-document summarization (MDS) is an effective tool for information aggregation that generates an in\\x02formative and concise summary from a cluster of topic-related documents. Our survey, the first of its kind,\\nsystematically overviews the recent deep-learning-based MDS models. We propose a novel taxonomy to sum\\x02marize the design strategies of neural networks and conduct a comprehensive summary of the state of the\\nart. We highlight the differences between various objective functions that are rarely discussed in the existing\\nliterature. Finally, we propose several future directions pertaining to this new and exciting field. In this article, we have presented the first comprehensive review of the most notable works to date\\non deep-learning-based multi-document summarization (MDS). We propose a taxonomy for orga\\x02nizing and clustering existing publications and devise the network design strategies based on the\\nstate-of-the-art methods. We also provide an overview of the existing multi-document objective\\nfunctions, evaluation metrics, and datasets and discuss some of the most pressing open problems\\nand promising future extensions in MDS research. We hope this survey provides readers with a\\ncomprehensive understanding of the key aspects of MDS tasks, clarifies the most notable advances,\\nand sheds light on future studies Multi-document Summarization via Deep Learning\\nTechniques: A SurveyMulti-document summarization (MDS), Deep learning models, Objective functions, Taxonomy, Evaluation metrics, Future directions, Information aggregationMulti-document summarization (MDS) is an effective tool for information aggregation that generates an in\\x02formative and concise summary from a cluster of topic-related documents. Our survey, the first of its kind,\\nsystematically overviews the recent deep-learning-based MDS models. We propose a novel taxonomy to sum\\x02marize the design strategies of neural networks and conduct a comprehensive summary of the state of the\\nart. We highlight the differences between various objective functions that are rarely discussed in the existing\\nliterature. Finally, we propose several future directions pertaining to this new and exciting field.In this article, we have presented the first comprehensive review of the most notable works to date\\non deep-learning-based multi-document summarization (MDS). We propose a taxonomy for orga\\x02nizing and clustering existing publications and devise the network design strategies based on the\\nstate-of-the-art methods. We also provide an overview of the existing multi-document objective\\nfunctions, evaluation metrics, and datasets and discuss some of the most pressing open problems\\nand promising future extensions in MDS research. We hope this survey provides readers with a\\ncomprehensive understanding of the key aspects of MDS tasks, clarifies the most notable advances,\\nand sheds light on future studieslla i aye RR | poe [Sena Sena | Sena, —+ ar ea) } (b) Word/Sentence-level Concatenation MethodsInput Extractive Summarization Hybrid Summarization Selective Selective | {Generative| Extractor; —e| Selective || input —e) Extractor —e| Sects [Abstractor —+| Generative Extractive Abstractive Model Abstractive Summarization —= Sree Seat avast —+{ Generative Summary. ak Generative) (Generative) sector —{ Sena [avstraror} (Ser } Abstractive - Abstractive Model /‘Dec {a) Naive Networks (c} Auxiliary Task Networks Processing earner} (b) Ensemble Networks =a ao (6) Reconstruction Networks elton Groh === {e) Fusion Networks Encoder oecoser Processing (f) Graph Neural Networks Processing |! {suman (g) Encoder-Decoder Structure a ocesine a a (h) Pre-trained Language Models Text summarization Natural Language Processing'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_str = test_data.loc[[0]]['Text'].iloc[0]\n",
    "text_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-03-22T16:13:10.881750Z",
     "iopub.status.busy": "2025-03-22T16:13:10.881338Z",
     "iopub.status.idle": "2025-03-22T16:13:20.405663Z",
     "shell.execute_reply": "2025-03-22T16:13:20.404918Z",
     "shell.execute_reply.started": "2025-03-22T16:13:10.881718Z"
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|>Below is an instruction that describes a task, Paired with an input\n",
      "### Instruction:\n",
      "Analyze the research article content and give me a summary from the research article. The summary should be as precise as possible and must include all the important points. Donot repeat information. Make the summary as crisp as possible\n",
      "\n",
      "### Input:\n",
      "Multi-document Summarization via Deep Learning\n",
      "Techniques: A Survey Multi-document summarization (MDS), Deep learning models, Objective functions, Taxonomy, Evaluation metrics, Future directions, Information aggregation Multi-document summarization (MDS) is an effective tool for information aggregation that generates an in\u0002formative and concise summary from a cluster of topic-related documents. Our survey, the first of its kind,\n",
      "systematically overviews the recent deep-learning-based MDS models. We propose a novel taxonomy to sum\u0002marize the design strategies of neural networks and conduct a comprehensive summary of the state of the\n",
      "art. We highlight the differences between various objective functions that are rarely discussed in the existing\n",
      "literature. Finally, we propose several future directions pertaining to this new and exciting field. In this article, we have presented the first comprehensive review of the most notable works to date\n",
      "on deep-learning-based multi-document summarization (MDS). We propose a taxonomy for orga\u0002nizing and clustering existing publications and devise the network design strategies based on the\n",
      "state-of-the-art methods. We also provide an overview of the existing multi-document objective\n",
      "functions, evaluation metrics, and datasets and discuss some of the most pressing open problems\n",
      "and promising future extensions in MDS research. We hope this survey provides readers with a\n",
      "comprehensive understanding of the key aspects of MDS tasks, clarifies the most notable advances,\n",
      "and sheds light on future studies Multi-document Summarization via Deep Learning\n",
      "Techniques: A SurveyMulti-document summarization (MDS), Deep learning models, Objective functions, Taxonomy, Evaluation metrics, Future directions, Information aggregationMulti-document summarization (MDS) is an effective tool for information aggregation that generates an in\u0002formative and concise summary from a cluster of topic-related documents. Our survey, the first of its kind,\n",
      "systematically overviews the recent deep-learning-based MDS models. We propose a novel taxonomy to sum\u0002marize the design strategies of neural networks and conduct a comprehensive summary of the state of the\n",
      "art. We highlight the differences between various objective functions that are rarely discussed in the existing\n",
      "literature. Finally, we propose several future directions pertaining to this new and exciting field.In this article, we have presented the first comprehensive review of the most notable works to date\n",
      "on deep-learning-based multi-document summarization (MDS). We propose a taxonomy for orga\u0002nizing and clustering existing publications and devise the network design strategies based on the\n",
      "state-of-the-art methods. We also provide an overview of the existing multi-document objective\n",
      "functions, evaluation metrics, and datasets and discuss some of the most pressing open problems\n",
      "and promising future extensions in MDS research. We hope this survey provides readers with a\n",
      "comprehensive understanding of the key aspects of MDS tasks, clarifies the most notable advances,\n",
      "and sheds light on future studieslla i aye RR | poe [Sena Sena | Sena, —+ ar ea) } (b) Word/Sentence-level Concatenation MethodsInput Extractive Summarization Hybrid Summarization Selective Selective | {Generative| Extractor; —e| Selective || input —e) Extractor —e| Sects [Abstractor —+| Generative Extractive Abstractive Model Abstractive Summarization —= Sree Seat avast —+{ Generative Summary. ak Generative) (Generative) sector —{ Sena [avstraror} (Ser } Abstractive - Abstractive Model /‘Dec {a) Naive Networks (c} Auxiliary Task Networks Processing earner} (b) Ensemble Networks =a ao (6) Reconstruction Networks elton Groh === {e) Fusion Networks Encoder oecoser Processing (f) Graph Neural Networks Processing |! {suman (g) Encoder-Decoder Structure a ocesine a a (h) Pre-trained Language Models Text summarization Natural Language Processing\n",
      "### Response: \n",
      "\n",
      "Analyze the research article content and give me a summary from the research article. The summary should be as precise as possible and must include all the important points. Donot repeat information. Make the summary as crisp as possible\n",
      "Multi-document Summarization via Deep Learning\n",
      "Techniques: A Survey Multi-document summarization (MDS), Deep learning models, Objective functions, Taxonomy, Evaluation metrics, Future directions, Information aggregation Multi-document summarization (MDS) is an effective tool for information aggregation that generates an in\u0002formative and concise summary from a cluster of topic-related documents. Our survey, the first of its kind,\n",
      "system\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import TextStreamer\n",
    "\n",
    "FastLanguageModel.for_inference(model)\n",
    "inputs = tokenizer(\n",
    "[\n",
    "    prompt.format(\n",
    "        text_str,\n",
    "        \"\"\n",
    "    )\n",
    "], return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "text_streamer = TextStreamer(tokenizer)\n",
    "_ = model.generate(**inputs, streamer=text_streamer, max_new_tokens = 128)\n",
    "generated_text = tokenizer.decode(inputs.input_ids[0], skip_special_tokens=True)\n",
    "\n",
    "# Extract the response by looking for the \"### Response:\" part\n",
    "response_start = generated_text.find(\"### Response:\") + len(\"### Response:\")\n",
    "response = generated_text[response_start:].strip()\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-22T17:55:53.540339Z",
     "iopub.status.busy": "2025-03-22T17:55:53.540002Z",
     "iopub.status.idle": "2025-03-22T17:56:03.141039Z",
     "shell.execute_reply": "2025-03-22T17:56:03.140119Z",
     "shell.execute_reply.started": "2025-03-22T17:55:53.540314Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task, Paired with an input\n",
      "### Instruction:\n",
      "Analyze the research article content and give me a summary from the research article. The summary should be as precise as possible and must include all the important points. Donot repeat information. Make the summary as crisp as possible\n",
      "\n",
      "### Input:\n",
      "Multi-document Summarization via Deep Learning\n",
      "Techniques: A Survey Multi-document summarization (MDS), Deep learning models, Objective functions, Taxonomy, Evaluation metrics, Future directions, Information aggregation Multi-document summarization (MDS) is an effective tool for information aggregation that generates an in\u0002formative and concise summary from a cluster of topic-related documents. Our survey, the first of its kind,\n",
      "systematically overviews the recent deep-learning-based MDS models. We propose a novel taxonomy to sum\u0002marize the design strategies of neural networks and conduct a comprehensive summary of the state of the\n",
      "art. We highlight the differences between various objective functions that are rarely discussed in the existing\n",
      "literature. Finally, we propose several future directions pertaining to this new and exciting field. In this article, we have presented the first comprehensive review of the most notable works to date\n",
      "on deep-learning-based multi-document summarization (MDS). We propose a taxonomy for orga\u0002nizing and clustering existing publications and devise the network design strategies based on the\n",
      "state-of-the-art methods. We also provide an overview of the existing multi-document objective\n",
      "functions, evaluation metrics, and datasets and discuss some of the most pressing open problems\n",
      "and promising future extensions in MDS research. We hope this survey provides readers with a\n",
      "comprehensive understanding of the key aspects of MDS tasks, clarifies the most notable advances,\n",
      "and sheds light on future studies Multi-document Summarization via Deep Learning\n",
      "Techniques: A SurveyMulti-document summarization (MDS), Deep learning models, Objective functions, Taxonomy, Evaluation metrics, Future directions, Information aggregationMulti-document summarization (MDS) is an effective tool for information aggregation that generates an in\u0002formative and concise summary from a cluster of topic-related documents. Our survey, the first of its kind,\n",
      "systematically overviews the recent deep-learning-based MDS models. We propose a novel taxonomy to sum\u0002marize the design strategies of neural networks and conduct a comprehensive summary of the state of the\n",
      "art. We highlight the differences between various objective functions that are rarely discussed in the existing\n",
      "literature. Finally, we propose several future directions pertaining to this new and exciting field.In this article, we have presented the first comprehensive review of the most notable works to date\n",
      "on deep-learning-based multi-document summarization (MDS). We propose a taxonomy for orga\u0002nizing and clustering existing publications and devise the network design strategies based on the\n",
      "state-of-the-art methods. We also provide an overview of the existing multi-document objective\n",
      "functions, evaluation metrics, and datasets and discuss some of the most pressing open problems\n",
      "and promising future extensions in MDS research. We hope this survey provides readers with a\n",
      "comprehensive understanding of the key aspects of MDS tasks, clarifies the most notable advances,\n",
      "and sheds light on future studieslla i aye RR | poe [Sena Sena | Sena, —+ ar ea) } (b) Word/Sentence-level Concatenation MethodsInput Extractive Summarization Hybrid Summarization Selective Selective | {Generative| Extractor; —e| Selective || input —e) Extractor —e| Sects [Abstractor —+| Generative Extractive Abstractive Model Abstractive Summarization —= Sree Seat avast —+{ Generative Summary. ak Generative) (Generative) sector —{ Sena [avstraror} (Ser } Abstractive - Abstractive Model /‘Dec {a) Naive Networks (c} Auxiliary Task Networks Processing earner} (b) Ensemble Networks =a ao (6) Reconstruction Networks elton Groh === {e) Fusion Networks Encoder oecoser Processing (f) Graph Neural Networks Processing |! {suman (g) Encoder-Decoder Structure a ocesine a a (h) Pre-trained Language Models Text summarization Natural Language Processing\n",
      "### Response: \n",
      "\n",
      "This article provides an overview of the most notable works on deep-learning-based multi-document summarization (MDS). The authors propose a taxonomy for organizing and clustering existing publications and devise the network design strategies based on the state-of-the-art methods. They also provide an overview of the existing multi-document objective functions, evaluation metrics, and datasets and discuss some of the most pressing open problems and promising future extensions in MDS research. The article highlights the differences between various objective functions that are rarely discussed in the existing literature and provides a comprehensive understanding of the key aspects of MDS tasks. The article is well-written and easy to follow, and it provides\n",
      "\n",
      "Final Response:\n",
      " This article provides an overview of the most notable works on deep-learning-based multi-document summarization (MDS). The authors propose a taxonomy for organizing and clustering existing publications and devise the network design strategies based on the state-of-the-art methods. They also provide an overview of the existing multi-document objective functions, evaluation metrics, and datasets and discuss some of the most pressing open problems and promising future extensions in MDS research. The article highlights the differences between various objective functions that are rarely discussed in the existing literature and provides a comprehensive understanding of the key aspects of MDS tasks. The article is well-written and easy to follow, and it provides\n"
     ]
    }
   ],
   "source": [
    "from transformers import TextStreamer\n",
    "\n",
    "def get_response_from_model(row, model, tokenizer, prompt_template):\n",
    "    # Prepare the input using the row's 'Text'\n",
    "    prompt = prompt_template.format(\n",
    "        row.iloc[0]['Text'],  # Properly extract text from the DataFrame\n",
    "        \"\"\n",
    "    )\n",
    "\n",
    "    # Tokenize and prepare inputs for the model\n",
    "    inputs = tokenizer([prompt], return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "    # # Set up the TextStreamer to process the model's output\n",
    "    # text_streamer = TextStreamer(tokenizer)\n",
    "\n",
    "    # Generate the response (this waits for model completion)\n",
    "    output_ids = model.generate(**inputs, max_new_tokens=128)\n",
    "\n",
    "    # Decode the generated output\n",
    "    generated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    print(generated_text)\n",
    "    # Extract the response after \"### Response:\"\n",
    "    response_start = generated_text.find(\"### Response:\") + len(\"### Response:\")\n",
    "    response = generated_text[response_start:].strip() if response_start != -1 else generated_text\n",
    "\n",
    "    return response\n",
    "\n",
    "# Call the function\n",
    "response = get_response_from_model(test_data.loc[[0]], trainer.model, tokenizer, prompt)\n",
    "print(\"\\nFinal Response:\\n\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-22T17:56:11.508920Z",
     "iopub.status.busy": "2025-03-22T17:56:11.508569Z",
     "iopub.status.idle": "2025-03-22T17:56:13.148376Z",
     "shell.execute_reply": "2025-03-22T17:56:13.147419Z",
     "shell.execute_reply.started": "2025-03-22T17:56:11.508889Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge1': 0.5340314136125655, 'rouge2': 0.2857142857142857, 'rougeL': 0.418848167539267, 'rougeLsum': 0.418848167539267}\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "\n",
    "# Load the ROUGE metric\n",
    "metric = evaluate.load(\"rouge\")\n",
    "\n",
    "# Ensure predictions and references are lists\n",
    "predictions = [response]  # Wrap response in a list\n",
    "references = [test_data.loc[[0]]['Summary'].values[0]]  # Wrap reference in a list\n",
    "\n",
    "# Add batch for evaluation\n",
    "metric.add_batch(predictions=predictions, references=references)\n",
    "\n",
    "# Compute the score\n",
    "score = metric.compute()\n",
    "\n",
    "# Return or print the score\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-22T18:20:20.760194Z",
     "iopub.status.busy": "2025-03-22T18:20:20.759846Z",
     "iopub.status.idle": "2025-03-22T18:38:09.329412Z",
     "shell.execute_reply": "2025-03-22T18:38:09.328436Z",
     "shell.execute_reply.started": "2025-03-22T18:20:20.760167Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of test data 112\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "\n",
      "Final ROUGE Score:\n",
      " {'rouge1': 0.5720591675557505, 'rouge2': 0.3409949041021048, 'rougeL': 0.42209499308107934, 'rougeLsum': 0.4218762224603859}\n"
     ]
    }
   ],
   "source": [
    "from transformers import TextStreamer\n",
    "import evaluate\n",
    "\n",
    "test_data=temp_data\n",
    "\n",
    "def get_response_from_model(row, model, tokenizer, prompt_template):\n",
    "    # Prepare the input using the row's 'Text'\n",
    "    prompt = prompt_template.format(\n",
    "        row['Text'],  # Extract text\n",
    "        \"\"\n",
    "    )\n",
    "\n",
    "    # Tokenize and prepare inputs for the model\n",
    "    inputs = tokenizer([prompt], return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "    # Generate the response\n",
    "    output_ids = model.generate(**inputs, max_new_tokens=128)\n",
    "\n",
    "    # Decode the generated output\n",
    "    generated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    # Extract response after \"### Response:\"\n",
    "    response_start = generated_text.find(\"### Response:\") + len(\"### Response:\")\n",
    "    response = generated_text[response_start:].strip() if response_start != -1 else generated_text\n",
    "\n",
    "    return response\n",
    "\n",
    "# Load ROUGE metric\n",
    "metric = evaluate.load(\"rouge\")\n",
    "print(\"Length of test data\",len(test_data))\n",
    "# List to store responses\n",
    "responses = []\n",
    "cnt=0;\n",
    "# Iterate over all rows in test_data\n",
    "for _, row in test_data.iterrows():\n",
    "    print(cnt);\n",
    "    cnt+=1;\n",
    "    response = get_response_from_model(row, trainer.model, tokenizer, prompt)\n",
    "    responses.append(response)\n",
    "\n",
    "# Ensure predictions and references are lists\n",
    "predictions = responses\n",
    "references = test_data[\"Summary\"].tolist()  # Convert 'Summary' column to list\n",
    "\n",
    "# Add batch for evaluation\n",
    "metric.add_batch(predictions=predictions, references=references)\n",
    "\n",
    "# Compute the ROUGE score\n",
    "score = metric.compute()\n",
    "\n",
    "# Print results\n",
    "print(\"\\nFinal ROUGE Score:\\n\", score)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 6936002,
     "sourceId": 11122567,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30918,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
